{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677134da-4cae-4fe8-ba2b-10e2aa37b0cc",
   "metadata": {},
   "source": [
    "### Cell 1: Import Libraries and Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fc7790-7fba-46d8-beb9-369b239c09f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Setup:\n",
      "Data directory: C:\\eu_ai_act_navigator\\data\n",
      "Expected PDF path: C:\\eu_ai_act_navigator\\data\\EU_AI_Act_latest.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = \"data\"\n",
    "PDF_NAME = \"EU_AI_Act_latest.pdf\"\n",
    "PDF_PATH = os.path.join(DATA_DIR, PDF_NAME)\n",
    "\n",
    "# Create data directory if it doesn't exist (good practice)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project Setup:\")\n",
    "print(f\"Data directory: {os.path.abspath(DATA_DIR)}\")\n",
    "print(f\"Expected PDF path: {os.path.abspath(PDF_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdd1e1-321b-4269-b25b-d1cf752ab199",
   "metadata": {},
   "source": [
    "### Cell 2: Load the PDF Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4cc5c5-53c8-4c87-871f-dcfe6d6f369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: data\\EU_AI_Act_latest.pdf\n",
      "Successfully loaded 144 pages from the PDF.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PDF_PATH):\n",
    "    print(f\"---\")\n",
    "    print(f\"ERROR: PDF not found at {PDF_PATH}.\")\n",
    "    print(f\"Please download the EU AI Act PDF, name it '{PDF_NAME}', and place it in the '{DATA_DIR}' directory.\")\n",
    "    print(f\"---\")\n",
    "    pages = [] # Initialize pages as empty list to prevent errors in subsequent cells\n",
    "else:\n",
    "    print(f\"Loading PDF from: {PDF_PATH}\")\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    try:\n",
    "        pages = loader.load() # This loads the PDF into a list of Document objects (one per page)\n",
    "        print(f\"Successfully loaded {len(pages)} pages from the PDF.\")\n",
    "\n",
    "        # Optional: Inspect the first few pages (uncomment to run)\n",
    "        # for i in range(min(3, len(pages))):\n",
    "        #     print(f\"\\n--- Content of Page {i+1} (first 300 chars) ---\")\n",
    "        #     print(pages[i].page_content[:300])\n",
    "        #     print(f\"--- Metadata of Page {i+1} ---\")\n",
    "        #     print(pages[i].metadata)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the PDF: {e}\")\n",
    "        pages = [] # Initialize pages as empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b3643-05c4-470b-bb44-bcec1cd8e1a2",
   "metadata": {},
   "source": [
    "### Cell 3: Split Documents into Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4edca97-89b8-470e-beb5-7c2630a9331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting the 144 pages into smaller chunks...\n",
      "Split the document into 792 chunks.\n"
     ]
    }
   ],
   "source": [
    "if 'pages' in locals() and pages: # Check if pages list was successfully created and is not empty\n",
    "    print(f\"\\nSplitting the {len(pages)} pages into smaller chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # The maximum number of characters in a chunk\n",
    "        chunk_overlap=150,      # Number of characters to overlap between chunks\n",
    "        length_function=len,    # How to measure chunk length (standard is len())\n",
    "        is_separator_regex=False, # We are not using regex separators here\n",
    "    )\n",
    "    docs_chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split the document into {len(docs_chunks)} chunks.\")\n",
    "\n",
    "    # Optional: Inspect the first few chunks (uncomment to run)\n",
    "    # if docs_chunks:\n",
    "    #     for i in range(min(3, len(docs_chunks))):\n",
    "    #         print(f\"\\n--- Chunk {i+1} (first 200 chars) ---\")\n",
    "    #         print(docs_chunks[i].page_content[:200])\n",
    "    #         print(f\"--- Metadata of Chunk {i+1} ---\")\n",
    "    #         # Metadata (like page number) is usually propagated from the original Document object\n",
    "    #         print(docs_chunks[i].metadata)\n",
    "    #         print(f\"Length of Chunk {i+1}: {len(docs_chunks[i].page_content)}\")\n",
    "else:\n",
    "    print(\"Variable 'pages' not defined or is empty. Please ensure the PDF was loaded successfully in the previous cell.\")\n",
    "    docs_chunks = [] # Initialize docs_chunks as empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e685b4-6d28-4716-9344-4f620c732425",
   "metadata": {},
   "source": [
    "### CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152c3ee3-62ce-48ad-a1d8-52ef3f4c264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n",
      "Current CUDA device: 0\n",
      "Device Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Ensure NVIDIA drivers and CUDA toolkit are installed correctly, and PyTorch was installed with CUDA support.\")\n",
    "    print(\"Falling back to CPU for embeddings. This will be slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2623c-2a5b-4571-97c5-4d2c723b0189",
   "metadata": {},
   "source": [
    "### Cell 4: Initialize Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dd45f4-306c-47f0-8082-424552c7707d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda for HuggingFaceEmbeddings.\n",
      "\n",
      "Initializing HuggingFaceEmbeddings model: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\eu_ai_act_navigator\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\eu_ai_act_navigator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\varun\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceEmbeddings model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # Updated import for newer Langchain\n",
    "\n",
    "# Determine device: 'cuda' if available, otherwise 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device} for HuggingFaceEmbeddings.\")\n",
    "\n",
    "# Choose an embedding model\n",
    "# 'all-MiniLM-L6-v2' is small, fast, and good for CPU/prototyping.\n",
    "# 'all-mpnet-base-v2' is larger, a bit slower, but generally better quality for GPU.\n",
    "# Given you have CUDA, let's default to a slightly better one if you like,\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" # Good quality, runs well on CUDA\n",
    "# model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" # Even better for QA tasks\n",
    "\n",
    "# model_kwargs ensure the model runs on the specified device\n",
    "model_kwargs = {'device': device}\n",
    "\n",
    "# encode_kwargs ensure that normalization is applied, good practice for many sentence-transformer models\n",
    "encode_kwargs = {'normalize_embeddings': True} # Often improves performance\n",
    "\n",
    "if 'docs_chunks' in locals() and docs_chunks: # Proceed only if chunks exist\n",
    "    print(f\"\\nInitializing HuggingFaceEmbeddings model: {model_name}\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    print(\"HuggingFaceEmbeddings model initialized successfully.\")\n",
    "\n",
    "    # Optional: Test embedding a single sentence (uncomment to run)\n",
    "    # text_to_embed = \"What are the requirements for high-risk AI systems?\"\n",
    "    # query_result = embeddings.embed_query(text_to_embed)\n",
    "    # print(f\"\\nTest embedding for '{text_to_embed}':\")\n",
    "    # print(f\"  First 5 dimensions: {query_result[:5]}\")\n",
    "    # print(f\"  Embedding length: {len(query_result)}\")\n",
    "else:\n",
    "    print(\"Variable 'docs_chunks' not defined or empty. Please ensure the document splitting cell ran successfully.\")\n",
    "    embeddings = None # Initialize as None to prevent errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600171fd-45a1-4b2b-86a5-aeab62707a6b",
   "metadata": {},
   "source": [
    "### Cell 5: Create and Save FAISS Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96c66db-f869-4eb7-82c8-f1e4b8c9e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating FAISS vector store with 792 chunks using 'sentence-transformers/all-mpnet-base-v2' embeddings...\n",
      "This may take a few minutes depending on the number of chunks and GPU speed...\n",
      "FAISS vector store created successfully in memory.\n",
      "FAISS index saved locally to: vectorstore/db_faiss_eu_ai_act\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss_eu_ai_act\" # Store in a subfolder for neatness\n",
    "\n",
    "if 'docs_chunks' in locals() and docs_chunks and 'embeddings' in locals() and embeddings:\n",
    "    print(f\"\\nCreating FAISS vector store with {len(docs_chunks)} chunks using '{model_name}' embeddings...\")\n",
    "    print(f\"This may take a few minutes depending on the number of chunks and GPU speed...\")\n",
    "\n",
    "    # Create the directory for the FAISS index if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(DB_FAISS_PATH), exist_ok=True)\n",
    "\n",
    "    # This step generates embeddings for all chunks and stores them.\n",
    "    db = FAISS.from_documents(docs_chunks, embeddings)\n",
    "    print(\"FAISS vector store created successfully in memory.\")\n",
    "\n",
    "    # Save the FAISS index locally so you don't have to rebuild it every time\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(f\"FAISS index saved locally to: {DB_FAISS_PATH}\")\n",
    "\n",
    "    # Optional: Test a similarity search (uncomment to run)\n",
    "    # print(\"\\nTesting similarity search in the newly created DB...\")\n",
    "    # test_query = \"What are the obligations for AI providers?\"\n",
    "    # # Note: For a real test, you'd load the DB first if not in memory,\n",
    "    # # but here 'db' is already in memory.\n",
    "    # search_results = db.similarity_search_with_score(test_query, k=2) # Get top 2 results\n",
    "    # print(f\"Search results for query: '{test_query}'\")\n",
    "    # for i, (doc, score) in enumerate(search_results):\n",
    "    #     print(f\"\\nResult {i+1} (Score: {score:.4f}):\") # Lower score is better for FAISS L2 distance\n",
    "    #     print(f\"  Source Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    #     print(f\"  Content (first 150 chars): {doc.page_content[:150]}...\")\n",
    "\n",
    "elif not ('docs_chunks' in locals() and docs_chunks):\n",
    "    print(\"Variable 'docs_chunks' not defined or empty. Run the document processing and chunking cells first.\")\n",
    "elif not ('embeddings' in locals() and embeddings):\n",
    "    print(\"Variable 'embeddings' not initialized. Run the embedding model initialization cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
