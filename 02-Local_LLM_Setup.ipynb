{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65dfb67-1838-4984-8a27-fcb1214c9b6d",
   "metadata": {},
   "source": [
    "#### Cell 6: Import Libraries and Define Core Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bb38c-9861-49a1-acae-3027051dc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Import Libraries and Define Core Configurations\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# --- Core Configurations ---\n",
    "# LLM Model ID for our VRAM-friendly choice\n",
    "LLM_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Determine device for LLM (CUDA if available, otherwise CPU)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Pinned library versions (ensure these are active in your environment)\n",
    "# Transformers: 4.40.1\n",
    "# Accelerate: 0.28.0\n",
    "# BitsAndBytes: Compatible version (e.g., 0.41.x, 0.42.x, or 0.43.0 for transformers 4.40.x)\n",
    "\n",
    "# Optional: Environment variable for PyTorch memory management.\n",
    "# Set this *before* the first PyTorch import if you suspect fragmentation issues,\n",
    "# though less likely to be needed with TinyLlama on 4GB VRAM.\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(f\"--- Phase 2: LLM Setup ---\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Target LLM: {LLM_MODEL_ID}\")\n",
    "\n",
    "# Verify library versions (optional but good for debugging)\n",
    "import transformers as hf_transformers # aliasing to avoid conflict with any 'transformers' variable\n",
    "import accelerate as acc\n",
    "import bitsandbytes as bnb\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {hf_transformers.__version__}\")\n",
    "print(f\"Accelerate version: {acc.__version__}\")\n",
    "print(f\"Bitsandbytes version: {bnb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d37ad-2e54-46b3-b4ad-71f71926aa4f",
   "metadata": {},
   "source": [
    "#### Cell 7: Configure 4-bit Quantization (if using CUDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc92a0-abe4-418f-8677-4598c7ec05cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Configure 4-bit Quantization (if using CUDA)\n",
    "\n",
    "bnb_config_llm = None  # Initialize to None\n",
    "dtype_to_use_llm = torch.float32 # Default for CPU or if CUDA fails\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"\\nConfiguring BitsAndBytes for 4-bit quantization on CUDA...\")\n",
    "    try:\n",
    "        bnb_config_llm = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,                # Enable loading model weights in 4-bit precision\n",
    "            bnb_4bit_quant_type=\"nf4\",        # Use \"nf4\" (NormalFloat4) quantization type for better precision\n",
    "            bnb_4bit_compute_dtype=torch.float16, # Computation will be done in float16 for speed and compatibility\n",
    "                                              # float16 is a safe choice for most GPUs, including older ones.\n",
    "            bnb_4bit_use_double_quant=False,  # Double quantization saves a tiny bit more VRAM but can be omitted for simplicity\n",
    "                                              # and for smaller models like TinyLlama.\n",
    "        )\n",
    "        dtype_to_use_llm = torch.float16 # Align with compute_dtype for consistency if needed by from_pretrained\n",
    "        print(\"BitsAndBytesConfig created successfully.\")\n",
    "        print(f\"  - load_in_4bit: {bnb_config_llm.load_in_4bit}\")\n",
    "        print(f\"  - bnb_4bit_compute_dtype: {bnb_config_llm.bnb_4bit_compute_dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error configuring BitsAndBytesConfig: {e}\")\n",
    "        print(\"Will attempt to load model on CUDA without BitsAndBytes quantization if it failed, or fallback to CPU.\")\n",
    "        bnb_config_llm = None # Fallback\n",
    "        # If BnB config fails, attempt to load in fp16 on CUDA without quantization\n",
    "        dtype_to_use_llm = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "\n",
    "elif DEVICE == \"cpu\":\n",
    "    print(\"\\nLLM will be loaded on CPU. BitsAndBytes 4-bit quantization is not applied for CPU.\")\n",
    "    # dtype_to_use_llm is already torch.float32\n",
    "\n",
    "if bnb_config_llm is None and DEVICE == \"cuda\":\n",
    "    print(\"Note: BitsAndBytesConfig failed or not applied. LLM will use more VRAM on CUDA (if it loads).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51e2a56-493e-4e83-8f72-16768e39f8ab",
   "metadata": {},
   "source": [
    "#### Cell 8: Load the TinyLlama Model and Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49102e-b34f-477a-8928-1dfe47eb87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load the TinyLlama Model and Tokenizer\n",
    "\n",
    "llm_model = None      # Initialize to None\n",
    "llm_tokenizer = None  # Initialize to None\n",
    "\n",
    "print(f\"\\nAttempting to load LLM: {LLM_MODEL_ID}\")\n",
    "\n",
    "try:\n",
    "    # Load Tokenizer first (generally safe and quick)\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True)\n",
    "    if llm_tokenizer.pad_token is None:\n",
    "        print(\"LLM Tokenizer pad_token is None. Setting to eos_token.\")\n",
    "        llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "        # Update model config if the tokenizer's pad_token_id was set based on eos_token_id\n",
    "        # This can be important for the model's understanding of padding during generation.\n",
    "        # However, we need the model object first to update its config. We'll do this after model load.\n",
    "    print(\"LLM tokenizer loaded successfully.\")\n",
    "\n",
    "    # Load Model\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(f\"Loading model on CUDA with quantization_config (if configured): {bnb_config_llm is not None}\")\n",
    "        model_kwargs_cuda = {\n",
    "            \"quantization_config\": bnb_config_llm, # Will be None if BnB config failed or not on CUDA\n",
    "            \"device_map\": {\"\": 0}, # Map all layers to GPU 0. Use DEVICE if it's \"cuda:X\"\n",
    "            \"trust_remote_code\": True,\n",
    "            \"attn_implementation\": \"eager\", # Safest attention implementation for compatibility\n",
    "            # Explicitly set torch_dtype if not using bnb_config_llm (e.g. fallback from BnB error)\n",
    "            # or if it helps stability. For BnB, compute_dtype in bnb_config should handle it.\n",
    "        }\n",
    "        if bnb_config_llm is None: # If BnB failed, try loading in fp16 without it\n",
    "             model_kwargs_cuda[\"torch_dtype\"] = torch.float16\n",
    "        \n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_ID, **model_kwargs_cuda)\n",
    "\n",
    "    elif DEVICE == \"cpu\":\n",
    "        print(\"Loading model on CPU...\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL_ID,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32 # Standard for CPU\n",
    "        )\n",
    "        llm_model.to(DEVICE) # Ensure it's on CPU if not by default\n",
    "\n",
    "    print(\"LLM model loaded successfully!\")\n",
    "\n",
    "    # If tokenizer's pad_token was set to eos_token, and model config doesn't have it, update model config.\n",
    "    # This needs to be done *after* the model is loaded.\n",
    "    if llm_tokenizer.pad_token == llm_tokenizer.eos_token and \\\n",
    "       llm_model is not None and \\\n",
    "       getattr(llm_model.config, 'pad_token_id', None) is None and \\\n",
    "       llm_tokenizer.eos_token_id is not None:\n",
    "        print(f\"Updating model's config.pad_token_id to tokenizer.eos_token_id ({llm_tokenizer.eos_token_id})\")\n",
    "        llm_model.config.pad_token_id = llm_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"---------------------------------------------\")\n",
    "    print(f\"ERROR loading LLM model or tokenizer: {e}\")\n",
    "    print(f\"---------------------------------------------\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    llm_model = None # Ensure model is None if loading failed\n",
    "    llm_tokenizer = None # Ensure tokenizer is None if loading failed\n",
    "\n",
    "if llm_model is not None and llm_tokenizer is not None:\n",
    "    print(f\"\\nModel {LLM_MODEL_ID} and its tokenizer are loaded and ready.\")\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(f\"Model is on CUDA. VRAM usage (approx for TinyLlama 4-bit): ~0.7-1.5GB + overheads.\")\n",
    "        print(\"Run nvidia-smi in a terminal to check actual VRAM usage.\")\n",
    "else:\n",
    "    print(f\"\\nFailed to load LLM {LLM_MODEL_ID} or its tokenizer. Subsequent cells may not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ce596-707d-4c9a-8416-f1ff69f47f3f",
   "metadata": {},
   "source": [
    "### Cell 9: Basic Test Generation (TinyLlama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a954d34-6dbc-430b-b912-efa2a1d3020b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 9: Basic Test Generation (TinyLlama)\n",
    "\n",
    "if llm_model is not None and llm_tokenizer is not None:\n",
    "    print(\"\\n--- Performing a basic test generation with TinyLlama Chat ---\")\n",
    "\n",
    "    # TinyLlama Chat uses a specific chat template.\n",
    "    messages_test = [\n",
    "        # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Optional system prompt for TinyLlama\n",
    "        {\"role\": \"user\", \"content\": \"What is the EU AI Act in simple terms?\"},\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Tokenize using the chat template\n",
    "        input_ids_test = llm_tokenizer.apply_chat_template(\n",
    "            messages_test,\n",
    "            add_generation_prompt=True, # Important for chat models\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE) # Move tokenized input to the same device as the model\n",
    "\n",
    "        print(f\"\\nUser Query: {messages_test[-1]['content']}\") # Get the last user message for print\n",
    "        print(f\"Input_ids shape: {input_ids_test.shape}\")\n",
    "\n",
    "        # Generate text\n",
    "        with torch.no_grad(): # Disable gradient calculations for inference\n",
    "            outputs_test = llm_model.generate(\n",
    "                input_ids_test,\n",
    "                max_new_tokens=100,       # Max new tokens to generate\n",
    "                temperature=0.6,          # Controls randomness. TinyLlama might need slight adjustment.\n",
    "                top_p=0.9,                # Nucleus sampling\n",
    "                do_sample=True,           # Enable sampling\n",
    "                pad_token_id=llm_tokenizer.eos_token_id # Crucial for consistent generation\n",
    "            )\n",
    "\n",
    "        # Decode only the newly generated part\n",
    "        # The output includes the prompt, so we slice it off.\n",
    "        # This assumes the generated sequence starts right after the input sequence.\n",
    "        response_tokens_test = outputs_test[0][input_ids_test.shape[1]:]\n",
    "        generated_text_test = llm_tokenizer.decode(response_tokens_test, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Alternative decoding (if the above doesn't strip prompt well for this model/template)\n",
    "        # full_decoded_text = llm_tokenizer.decode(outputs_test[0], skip_special_tokens=True)\n",
    "        # print(f\"Full Decoded Output (for debugging template):\\n{full_decoded_text}\")\n",
    "        # if full_decoded_text.startswith(llm_tokenizer.decode(input_ids_test[0], skip_special_tokens=True)):\n",
    "        #     # This is a bit fragile if special tokens in prompt are skipped differently\n",
    "        #     pass\n",
    "\n",
    "\n",
    "        print(f\"Generated Text (TinyLlama):\\n'{generated_text_test}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        print(f\"Error during TinyLlama test generation: {e}\")\n",
    "        print(f\"--------------------------------------------------\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nTinyLlama model or tokenizer not loaded. Cannot perform test generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f3c10-2b35-4f5e-ace2-59e29721b83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
