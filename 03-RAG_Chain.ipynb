{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66de8a7c-b074-4c7d-b664-06569818c3ff",
   "metadata": {},
   "source": [
    "### Cell 10: Import Libraries for RAG and Define Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077a8354-9338-4db1-87e7-9794998a02a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 3: RAG Chain - TinyLlama & Embeddings on CUDA (Optimized) ---\n",
      "Targeting device: cuda:0\n",
      "LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Embedding Model for FAISS: sentence-transformers/all-mpnet-base-v2\n",
      "FAISS DB path: vectorstore/db_faiss_eu_ai_act\n",
      "Chunks to retrieve (k): 5\n",
      "Max new tokens for LLM: 190\n",
      "Initial CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Imports, Core Configurations, and Library Version Checks\n",
    "import os\n",
    "import torch\n",
    "import gc # For garbage collection\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# LangChain components\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "# from langchain_core.messages import SystemMessage, HumanMessage, AIMessage # Not directly used in final chain\n",
    "\n",
    "print(f\"--- Phase 3: RAG Chain - TinyLlama & Embeddings on CUDA (Optimized) ---\")\n",
    "\n",
    "# --- Core Configurations ---\n",
    "LLM_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "EMBEDDING_MODEL_NAME_FOR_FAISS = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss_eu_ai_act\" # Ensure this path is correct\n",
    "NUM_CHUNKS_TO_RETRIEVE = 5 # CRITICAL: Start with k=1 for minimal context to save VRAM\n",
    "MAX_NEW_TOKENS_LLM = 190   # CRITICAL: Keep LLM output short to save KV cache VRAM\n",
    "\n",
    "# Device Configuration\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"CUDA is not available. This script is configured to run all components on CUDA.\")\n",
    "DEVICE = \"cuda\"\n",
    "CUDA_DEVICE_ID = 0 # Assuming GPU 0\n",
    "torch.cuda.set_device(CUDA_DEVICE_ID) # Explicitly set default CUDA device for PyTorch\n",
    "\n",
    "print(f\"Targeting device: {DEVICE}:{CUDA_DEVICE_ID}\")\n",
    "print(f\"LLM: {LLM_MODEL_ID}\")\n",
    "print(f\"Embedding Model for FAISS: {EMBEDDING_MODEL_NAME_FOR_FAISS}\")\n",
    "print(f\"FAISS DB path: {DB_FAISS_PATH}\")\n",
    "print(f\"Chunks to retrieve (k): {NUM_CHUNKS_TO_RETRIEVE}\")\n",
    "print(f\"Max new tokens for LLM: {MAX_NEW_TOKENS_LLM}\")\n",
    "\n",
    "\n",
    "# Clear cache at the beginning of the session\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Initial CUDA cache cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea1586-af8b-4531-9c65-be59a16ac84f",
   "metadata": {},
   "source": [
    "### Cell 11: Configure and Load TinyLlama Model & Tokenizer \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c8e843-15a5-4a66-8e26-b2cd252c0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring BitsAndBytes for 4-bit LLM quantization on CUDA...\n",
      "BitsAndBytesConfig for LLM created successfully.\n",
      "\n",
      "Attempting to load LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0 on cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\eu_ai_act_navigator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0619 14:23:39.670000 12668 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\eu_ai_act_navigator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model loaded successfully!\n",
      "Model config.pad_token_id updated to 2.\n",
      "\n",
      "LLM TinyLlama/TinyLlama-1.1B-Chat-v1.0 loaded. Device: cuda:0.\n",
      "CUDA memory allocated: 789.51 MB\n",
      "CUDA memory reserved: 836.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Configure and Load TinyLlama Model & Tokenizer (Optimized for CUDA)\n",
    "\n",
    "llm_model = None\n",
    "llm_tokenizer = None\n",
    "bnb_config_llm = None\n",
    "dtype_to_use_llm = torch.float16 # Defaulting to float16 for CUDA with BnB\n",
    "\n",
    "print(\"\\nConfiguring BitsAndBytes for 4-bit LLM quantization on CUDA...\")\n",
    "try:\n",
    "    bnb_config_llm = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16, # float16 for compute with TinyLlama\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "    print(\"BitsAndBytesConfig for LLM created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not configure BitsAndBytesConfig for LLM: {e}\")\n",
    "    print(\"This is essential for running on limited VRAM. Halting.\")\n",
    "    # Raise an error or exit if BnB cannot be configured for CUDA\n",
    "    raise SystemExit(\"BitsAndBytesConfig failed, cannot proceed with GPU LLM loading.\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to load LLM: {LLM_MODEL_ID} on {DEVICE}:{CUDA_DEVICE_ID}\")\n",
    "try:\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, trust_remote_code=True)\n",
    "    if llm_tokenizer.pad_token is None:\n",
    "        llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "        print(f\"LLM Tokenizer pad_token set to eos_token ({llm_tokenizer.eos_token_id}).\")\n",
    "    print(\"LLM tokenizer loaded successfully.\")\n",
    "\n",
    "    model_load_kwargs = {\n",
    "        \"quantization_config\": bnb_config_llm,\n",
    "        \"device_map\": {\"\": CUDA_DEVICE_ID}, # Pin model to the specified CUDA device\n",
    "        \"trust_remote_code\": True,\n",
    "        \"attn_implementation\": \"eager\", # Most stable attention\n",
    "        # No explicit torch_dtype here, bnb_config's compute_dtype and device_map handle it\n",
    "    }\n",
    "    \n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_ID, **model_load_kwargs)\n",
    "    print(\"LLM model loaded successfully!\")\n",
    "\n",
    "    if llm_tokenizer.pad_token == llm_tokenizer.eos_token and \\\n",
    "       getattr(llm_model.config, 'pad_token_id', None) is None and \\\n",
    "       llm_tokenizer.eos_token_id is not None:\n",
    "        llm_model.config.pad_token_id = llm_tokenizer.eos_token_id\n",
    "        print(f\"Model config.pad_token_id updated to {llm_tokenizer.eos_token_id}.\")\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    llm_model.eval()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"---------------------------------------------\")\n",
    "    print(f\"FATAL ERROR loading LLM or tokenizer: {e}\")\n",
    "    print(f\"---------------------------------------------\")\n",
    "    import traceback; traceback.print_exc()\n",
    "    llm_model = None; llm_tokenizer = None\n",
    "    raise SystemExit(\"LLM loading failed.\")\n",
    "\n",
    "\n",
    "if llm_model and llm_tokenizer:\n",
    "    print(f\"\\nLLM {LLM_MODEL_ID} loaded. Device: {llm_model.device}.\")\n",
    "    # Quick VRAM check after model load\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(CUDA_DEVICE_ID)/1024**2:.2f} MB\")\n",
    "        print(f\"CUDA memory reserved: {torch.cuda.memory_reserved(CUDA_DEVICE_ID)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\nFailed to load LLM {LLM_MODEL_ID} or its tokenizer. Halting.\")\n",
    "    raise SystemExit(\"LLM or Tokenizer failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35484dd-f423-4c43-88ee-68fb6e625d7f",
   "metadata": {},
   "source": [
    "### Cell 12: Load FAISS Vector Store and Initialize Retriever \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ac0019-5abd-48cd-8d75-ef6172456fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FAISS vector store from: vectorstore/db_faiss_eu_ai_act\n",
      "Attempting to load embedding model (sentence-transformers/all-mpnet-base-v2) for FAISS on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\eu_ai_act_navigator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model for FAISS retrieval loaded successfully on cuda.\n",
      "FAISS vector store loaded successfully.\n",
      "Retriever created. Will retrieve top 5 document chunk(s).\n",
      "\n",
      "FAISS DB and Retriever are ready.\n",
      "After FAISS load - CUDA memory allocated: 1209.73 MB\n",
      "After FAISS load - CUDA memory reserved: 1286.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Load FAISS Vector Store and Initialize Retriever (Embeddings on CUDA)\n",
    "\n",
    "db_retriever = None\n",
    "embedding_model_for_retrieval = None\n",
    "\n",
    "print(f\"\\nLoading FAISS vector store from: {DB_FAISS_PATH}\")\n",
    "if not os.path.exists(DB_FAISS_PATH):\n",
    "    print(f\"FATAL ERROR: FAISS index not found at {DB_FAISS_PATH}. Please ensure Phase 1 was completed.\")\n",
    "    raise SystemExit(\"FAISS index not found.\")\n",
    "\n",
    "try:\n",
    "    print(f\"Attempting to load embedding model ({EMBEDDING_MODEL_NAME_FOR_FAISS}) for FAISS on device: {DEVICE}\")\n",
    "    embedding_model_for_retrieval = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME_FOR_FAISS,\n",
    "        model_kwargs={'device': DEVICE}, # <<< EMBEDDINGS ON CUDA\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(f\"Embedding model for FAISS retrieval loaded successfully on {DEVICE}.\")\n",
    "\n",
    "    # Set model to evaluation mode (good practice for sentence-transformers models too)\n",
    "    if hasattr(embedding_model_for_retrieval, 'client') and hasattr(embedding_model_for_retrieval.client, 'eval'):\n",
    "        embedding_model_for_retrieval.client.eval()\n",
    "\n",
    "\n",
    "    faiss_db = FAISS.load_local(\n",
    "        DB_FAISS_PATH,\n",
    "        embedding_model_for_retrieval,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"FAISS vector store loaded successfully.\")\n",
    "\n",
    "    db_retriever = faiss_db.as_retriever(search_kwargs={\"k\": NUM_CHUNKS_TO_RETRIEVE})\n",
    "    print(f\"Retriever created. Will retrieve top {db_retriever.search_kwargs['k']} document chunk(s).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"FATAL ERROR loading FAISS DB or initializing retriever: {e}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    import traceback; traceback.print_exc()\n",
    "    db_retriever = None\n",
    "    raise SystemExit(\"FAISS/Retriever loading failed.\")\n",
    "\n",
    "if db_retriever:\n",
    "    print(\"\\nFAISS DB and Retriever are ready.\")\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "        print(f\"After FAISS load - CUDA memory allocated: {torch.cuda.memory_allocated(CUDA_DEVICE_ID)/1024**2:.2f} MB\")\n",
    "        print(f\"After FAISS load - CUDA memory reserved: {torch.cuda.memory_reserved(CUDA_DEVICE_ID)/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"Retriever not initialized. Halting.\")\n",
    "    raise SystemExit(\"Retriever initialization failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b17a2-7f79-42d3-8a5a-2eaacd5ff15b",
   "metadata": {},
   "source": [
    "### Cell 13: Define RAG Prompt and Custom LLM Interaction Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed13a080-e8a4-4e59-8069-453b3065d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempting to Define RAG Components in Cell 13 (Simplified & Enhanced) ---\n",
      "LLM Model, Tokenizer, MAX_NEW_TOKENS_LLM, and DEVICE found. Proceeding...\n",
      "Enhanced RAG message preparation runnable defined successfully.\n",
      "Custom LLM invoker runnable with enhanced parameters defined successfully.\n",
      "\n",
      "Status after Cell 13 execution attempts:\n",
      "  prepare_messages_runnable is defined: True\n",
      "  custom_llm_invoker is defined: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Define RAG Prompt, Message Preparer, and Custom LLM Invoker (Simplified & Enhanced)\n",
    "\n",
    "# Initialize to None so they are defined in this cell's scope\n",
    "prepare_messages_runnable = None\n",
    "custom_llm_invoker = None\n",
    "# rag_prompt_for_tinyllama is not directly used in the chain if prepare_messages_runnable handles all formatting.\n",
    "\n",
    "print(\"\\n--- Attempting to Define RAG Components in Cell 13 (Simplified & Enhanced) ---\")\n",
    "\n",
    "# Crucial Check: Ensure LLM, Tokenizer, MAX_NEW_TOKENS_LLM, and DEVICE from earlier cells are loaded\n",
    "if 'llm_model' in locals() and llm_model is not None and \\\n",
    "   'llm_tokenizer' in locals() and llm_tokenizer is not None and \\\n",
    "   'MAX_NEW_TOKENS_LLM' in locals() and isinstance(MAX_NEW_TOKENS_LLM, int) and \\\n",
    "   'DEVICE' in locals() and DEVICE is not None:\n",
    "\n",
    "    print(\"LLM Model, Tokenizer, MAX_NEW_TOKENS_LLM, and DEVICE found. Proceeding...\")\n",
    "\n",
    "    # 1. Define the Enhanced System Prompt (Content only)\n",
    "    enhanced_system_prompt_content_str = \"\"\"You are an AI assistant. Your sole task is to answer the user's question based *only* on the provided 'Context from EU AI Act'.\n",
    "Be concise and factual. Synthesize the information from the context into a coherent answer.\n",
    "If the context does not contain enough information to answer the question, you MUST state: 'The provided context does not contain sufficient information to answer this question.'\n",
    "Do not use any external knowledge or make assumptions beyond the context. Output only the answer.\"\"\"\n",
    "    # Removed <|system|> and </s> as apply_chat_template will handle based on role dicts.\n",
    "\n",
    "    # 2. Define the Enhanced Message Preparation Function\n",
    "    def prepare_rag_messages_enhanced(input_dict: dict) -> list:\n",
    "        context_str = input_dict[\"context\"]\n",
    "        user_question_str = input_dict[\"question\"]\n",
    "\n",
    "        user_content_str = f\"\"\"Here is some relevant context from the EU AI Act:\n",
    "--------------------\n",
    "{context_str}\n",
    "--------------------\n",
    "Considering *only* this context, please answer the following question: {user_question_str}\n",
    "Your answer should be a synthesized, natural language response based on the context.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": enhanced_system_prompt_content_str},\n",
    "            {\"role\": \"user\", \"content\": user_content_str}\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "    prepare_messages_runnable = RunnableLambda(prepare_rag_messages_enhanced)\n",
    "    print(\"Enhanced RAG message preparation runnable defined successfully.\")\n",
    "\n",
    "\n",
    "    # 3. Define the LLM Invocation with Adjusted Parameters\n",
    "    # This function will use llm_model, llm_tokenizer, MAX_NEW_TOKENS_LLM, DEVICE from the outer scope (Cell 10 & 11)\n",
    "    def invoke_llm_and_decode_simple(messages_for_llm: list) -> str:\n",
    "        # No need to check for llm_model/tokenizer here again if the outer 'if' passed.\n",
    "        # They are accessible from the scope where this function is defined and used by the Lambda.\n",
    "        try:\n",
    "            input_ids = llm_tokenizer.apply_chat_template(\n",
    "                messages_for_llm,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(llm_model.device) # Use the device the model is actually on\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = llm_model.generate(\n",
    "                    input_ids,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS_LLM, # From Cell 10\n",
    "                    temperature=0.35,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=llm_tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.15\n",
    "                )\n",
    "            \n",
    "            response_start_index = input_ids.shape[1]\n",
    "            generated_token_ids = output_ids[0][response_start_index:]\n",
    "            response_text = llm_tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            if not response_text:\n",
    "                return \"The model generated an empty response.\"\n",
    "            return response_text\n",
    "        except Exception as e_gen:\n",
    "            current_error_msg = f\"ERROR during LLM generation: {e_gen}\"\n",
    "            print(current_error_msg)\n",
    "            # import traceback; traceback.print_exc() # Uncomment for full traceback\n",
    "            return f\"ERROR in generation: {str(e_gen)[:200]}\"\n",
    "\n",
    "    custom_llm_invoker = RunnableLambda(invoke_llm_and_decode_simple) # Changed function name\n",
    "    print(\"Custom LLM invoker runnable with enhanced parameters defined successfully.\")\n",
    "\n",
    "else:\n",
    "    print(\"Prerequisite variables (llm_model, llm_tokenizer, MAX_NEW_TOKENS_LLM, or DEVICE) from earlier cells not found, not loaded, or of incorrect type.\")\n",
    "    print(\"RAG components in Cell 13 were NOT created.\")\n",
    "    # Print status of each prerequisite to help debug\n",
    "    print(f\"  llm_model is loaded: {'llm_model' in locals() and llm_model is not None}\")\n",
    "    print(f\"  llm_tokenizer is loaded: {'llm_tokenizer' in locals() and llm_tokenizer is not None}\")\n",
    "    print(f\"  MAX_NEW_TOKENS_LLM is defined as int: {'MAX_NEW_TOKENS_LLM' in locals() and isinstance(MAX_NEW_TOKENS_LLM, int)}\")\n",
    "    print(f\"  DEVICE is defined: {'DEVICE' in locals() and DEVICE is not None}\")\n",
    "\n",
    "\n",
    "# Final Sanity check print after definition attempts\n",
    "print(f\"\\nStatus after Cell 13 execution attempts:\")\n",
    "print(f\"  prepare_messages_runnable is defined: {prepare_messages_runnable is not None}\")\n",
    "print(f\"  custom_llm_invoker is defined: {custom_llm_invoker is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78b42d-dafc-4274-a33f-1335fa86dc8c",
   "metadata": {},
   "source": [
    "### Cell 14: Construct and Test the RAG Chain with TinyLlama \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40dbba2b-a60e-4800-94a2-7611d8d700a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constructing the final RAG chain with TinyLlama (Simplified)...\n",
      "Final RAG chain constructed successfully!\n",
      "\n",
      "--- Testing the Final RAG Chain (Simplified) ---\n",
      "\n",
      "--- RAG Test Question 1: What are prohibited AI practices according to the EU AI Act? ---\n",
      "TinyLlama RAG Chain Response:\n",
      "'According to the EU AI Act, the prohibited AI practices include the placing on the market, the putting into service or the use of an artificial intelligence (AI) system that uses subliminal techniques or manipulates behavior in a way that materially distorts the abilities of individuals or groups. These practices aim to manipulate or deceive people or groups for the sake of achieving specific goals, such as material gains or political power. Therefore, these practices violate the principles of fairness, transparency, and consent required by the act.'\n",
      "\n",
      "--- RAG Test Question 2: How is 'AI system' defined in the act? ---\n",
      "TinyLlama RAG Chain Response:\n",
      "'In the European Union AI Act, \"AI system\" is defined as a machine-based system that is designed to operate with varying levels of autonomy and exhibits adaptive behavior that can generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. An AI system is deemed high-risk where it poses a significant risk of harm to health, safety, or fundamental rights of natural persons, including by not materially influencing the outcome of decision-making. In addition, an AI system is considered high-risk where it is intended to perform a previously conducted human activity, without proper human review, or when it is intended to perform a preparatory task for an assessment relevant to the purposes of using the AI system. Changes to algorithms and the ability to adapt automatically to functioning, which continue to \"learn,\" do not constitute substantial modifications,'\n",
      "\n",
      "--- RAG Test Question 3: What does the act say about transparency for high-risk systems? ---\n",
      "TinyLlama RAG Chain Response:\n",
      "'Yes, the act says that AI systems that are considered high risk should be required to register themselves and provide information about them in an EU database called \"High Risk Use Cases\". Deployers of such high-risk AI systems should voluntarily register themselves in the database and provide the system they intend to use. The text also mentions that other deployers may have access to the database freely, but the information should be available for public viewing. Overall, the act emphasizes the importance of transparency when it comes to AI systems that pose risks to human rights and democratic values.'\n",
      "\n",
      "--- RAG Test Question 4: What is the airspeed velocity of an unladen swallow? ---\n",
      "TinyLlama RAG Chain Response:\n",
      "'Based on the given context, \"The provided context does not contain sufficient information to answer this question.\" The Airspeed Velocity of an Unladen Swallow is approximately 50 km/h (31 miles per hour).'\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Construct and Test the Final RAG Chain (Simplified)\n",
    "\n",
    "# Helper function (ensure this is defined if not already in scope)\n",
    "def format_retrieved_docs(docs: list) -> str:\n",
    "    return \"\\n\".join([f\"Snippet from Page {d.metadata.get('page', 'N/A')}:\\n{d.page_content}\" for d in docs])\n",
    "\n",
    "rag_chain_final = None\n",
    "\n",
    "# Check that all components from previous cells are not None\n",
    "if 'db_retriever' in locals() and db_retriever and \\\n",
    "   'prepare_messages_runnable' in locals() and prepare_messages_runnable and \\\n",
    "   'custom_llm_invoker' in locals() and custom_llm_invoker:\n",
    "\n",
    "    print(\"\\nConstructing the final RAG chain with TinyLlama (Simplified)...\")\n",
    "    try:\n",
    "        rag_chain_final = (\n",
    "            { # Step 1: Prepare input for message formatting\n",
    "                \"context\": RunnablePassthrough() | db_retriever | format_retrieved_docs,\n",
    "                \"question\": RunnablePassthrough()\n",
    "            }\n",
    "            | prepare_messages_runnable # Step 2: Output: list of messages\n",
    "            | custom_llm_invoker        # Step 3: Output: string (LLM response)\n",
    "            | StrOutputParser()         # Step 4: Ensure output is a string\n",
    "        )\n",
    "        print(\"Final RAG chain constructed successfully!\")\n",
    "\n",
    "        # --- Testing Loop (same as before) ---\n",
    "        print(\"\\n--- Testing the Final RAG Chain (Simplified) ---\")\n",
    "        test_questions_final_rag = [\n",
    "            \"What are prohibited AI practices according to the EU AI Act?\",\n",
    "            \"How is 'AI system' defined in the act?\",\n",
    "            \"What does the act say about transparency for high-risk systems?\",\n",
    "            \"What is the airspeed velocity of an unladen swallow?\"\n",
    "        ]\n",
    "        for i, user_query in enumerate(test_questions_final_rag):\n",
    "            print(f\"\\n--- RAG Test Question {i+1}: {user_query} ---\")\n",
    "            if DEVICE == \"cuda\" and i > 0:\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "            try:\n",
    "                llm_response = rag_chain_final.invoke(user_query)\n",
    "                print(f\"TinyLlama RAG Chain Response:\\n'{llm_response}'\")\n",
    "            except Exception as e_invoke:\n",
    "                print(f\"Error during RAG chain invocation: {e_invoke}\")\n",
    "                # import traceback; traceback.print_exc() # Uncomment for full trace\n",
    "                if \"out of memory\" in str(e_invoke).lower():\n",
    "                    print(\"CUDA Out of Memory. Halting.\"); break\n",
    "        \n",
    "        # Final Cleanup (same as before)\n",
    "        # ... (your cleanup code) ...\n",
    "\n",
    "    except Exception as e_chain_const:\n",
    "        print(f\"Error constructing the final RAG chain: {e_chain_const}\")\n",
    "        # import traceback; traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nCannot construct final RAG chain due to missing components. Check status from Cell 13 output:\")\n",
    "    print(f\"  db_retriever is OK: {'db_retriever' in locals() and db_retriever is not None}\")\n",
    "    print(f\"  prepare_messages_runnable is OK: {'prepare_messages_runnable' in locals() and prepare_messages_runnable is not None}\")\n",
    "    print(f\"  custom_llm_invoker is OK: {'custom_llm_invoker' in locals() and custom_llm_invoker is not None}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
